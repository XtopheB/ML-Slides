%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  This Beamer template was created by Cameron Bracken.

\documentclass[xcolor=x11names,compress, handhouts]{beamer}
%% General document
\usepackage{graphicx, subfig}
%% Beamer Layout
\useoutertheme[subsection=false,shadow]{miniframes}
\useinnertheme{default}
\usefonttheme{serif}
\usepackage{palatino}

%%%%%%% Mes Packages %%%%%%%%%%%%%%%%
%\usepackage[french]{babel}
\usepackage[T1]{fontenc}
\usepackage{color}
\usepackage{xcolor}
\usepackage{dsfont} % Pour indicatrice
\usepackage{url}
\usepackage{multirow}

%remove the icons
\setbeamertemplate{bibliography item}{}

%remove line breaks
\setbeamertemplate{bibliography entry title}{}
\setbeamertemplate{bibliography entry location}{}
\setbeamertemplate{bibliography entry note}{}

%% ------ MEs couleurs --------
\definecolor{vert}{rgb}{0.1,0.7,0.2}
\definecolor{brique}{rgb}{0.7,0.16,0.16}
\definecolor{gris}{rgb}{0.7, 0.75, 0.71}
\definecolor{twitterblue}{rgb}{0, 0.42, 0.58}
\definecolor{airforceblue}{rgb}{0.36, 0.54, 0.66}
\definecolor{siap}{RGB}{3,133, 168}


%%%%%%%%%%%%%%%%% BEAMER PACKAGE %%%%%%%

\setbeamerfont{title like}{shape=\scshape}
\setbeamerfont{frametitle}{shape=\scshape}

\setbeamercolor*{lower separation line head}{bg=DeepSkyBlue4}
\setbeamercolor*{normal text}{fg=black,bg=white}
\setbeamercolor*{alerted text}{fg=red}
\setbeamercolor*{example text}{fg=black}
\setbeamercolor*{structure}{fg=black}
\setbeamercolor*{palette tertiary}{fg=black,bg=black!10}
\setbeamercolor*{palette quaternary}{fg=black,bg=black!10}

\renewcommand{\(}{\begin{columns}}
\renewcommand{\)}{\end{columns}}
\newcommand{\<}[1]{\begin{column}{#1}}
\renewcommand{\>}{\end{column}}

% Path for the graphs
\graphicspath{{Graphics/}{../../../../Visualisation/Presentations/Graphics/}{../../Visualisation/Presentations/Graphics/}
{c:/Gitmain/MLCourse/UNML/Module4/M4-0-SimpleTrees_files/figure-html/}
{c:/Gitmain/MLCourse/UNML/Module4/M4-1-DecisionTrees_files/figure-html/}
{c:/Gitmain/MLCourse/UNML/Module2/M2-1-SimpleClassification_files/figure-html/}
{c:/Gitmain/MLCourse/UNML/Module4/M4-2-RandomForest_files/figure-html/}
}

%remove navigation symbols
\setbeamertemplate{navigation symbols}{}


% Natbib for clean bibliography
\usepackage[comma,authoryear]{natbib}

\begin{document}
%%% Title page %%%%%
\begin{frame}
\Large{ \color{siap}{Machine Learning for Official Statistics \& SDGs}}

\hspace{1cm}

\color{brique}{\huge{Random Forest}}

\hspace{2cm}
\begin{center}

\includegraphics[height=0.10\textwidth]{SIAP_logo_Big.png}

\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/153-penalized-regression-essentials-ridge-lasso-elastic-net/
%%

%%%  ----------- Dataviz  Definition  -----------%%%
\section{Introduction}


\begin{frame} % Cover slide
\frametitle{\textcolor{brique}{[  From Trees to Forest  ]}}
\textit{Trees} are methods for classification or regression analysis.\\
\begin{center} \includegraphics[width = 0.5\textwidth]{rparttree-1.png} \end{center}
\pause
\begin{itemize}
  \item Trees are based on  recursive binary splits
  \item The structure is simple and corresponds to regions in the variable's space
  \item Each node is based on a the value of one variable  and a threshold
  \item Trees can be very detailed and prone to overfitting
\end{itemize}
\end{frame}

\section{The problem with trees}

\begin{frame} % Cover slide
\frametitle{\textcolor{brique}{[ Problems with  trees ]}}
\pause
\begin{itemize}
  \item[] By their structure
  \begin{itemize}[<+->]
    \item Trees are splitting the variable's space into "rectangles"
    \item Predictions using trees are not very accurate
    \item Trees are not robust to changes in the data
    item  Trees are prone to overfiting
  \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{\textcolor{brique}{[ Problems with  trees  ]}}
The decision trees  suffer from \textit{high variance}:
\pause
\begin{itemize}[<+->]
    \item Take a sample and build a tree
    \item Divide the sample in two parts and build a tree on each part
    \item[$\hookrightarrow$] The two trees will likely be very different
\end{itemize}
\end{frame}

\begin{frame} % Cover slide
\frametitle{\textcolor{brique}{[ Example on a simple tree ]}}
\pause
\begin{itemize}
\item[] Which tree do you trust? \\
   \only<2> {\includegraphics[width = 0.8\textwidth]{treesvariance-1.png} \\ }
   \only<2> {Tree using all observations}
   \only<3> {\includegraphics[width = 0.8\textwidth]{treesvariance-2.png} \\ }
   \only<3> {First tree with 1/2 of observations}
   \only<4> {\includegraphics[width = 0.8\textwidth]{treesvariance-3.png} \\ }
   \only<4> {Second tree with 1/2 of observations}
   \only<5> {Outcomes are different from one tree to another\\ }
   \only<5> {$\hookrightarrow$ Instability of the outcome ( or high variance)}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{\textcolor{brique}{[ How to use trees? ]}}
Alternative to simple tree models:
\pause
\begin{itemize}[<+->]
    \item Aggregating the results of trees can help
    \item[$\hookrightarrow$]  "\emph{Bagging}"
    \item Some trees may be too similar (correlated) and have to be "decorrelated" using random draws of the variable used
    \item[$\hookrightarrow$]  "\emph{Random forest}"
    \item Trees can be constructed \textit{sequentially}
    \item[$\hookrightarrow$] "\emph{Boosting}"
\end{itemize}
\end{frame}



\section{Bagging}

\begin{frame}
\frametitle{\textcolor{brique}{[Some simple mathematical tricks ]}}
Aggregating the results of trees can help decrease the variance % \\ A bit of maths won't hurt:
\pause
\begin{itemize}[<+->]
    \item  Intuitively, if $U_{i}, \ldots U_{B}$ are  iid with variance $\sigma^{2}$, then the average of these variables has a lower variance
    \item[]
\begin{eqnarray*}
Var ( \overline{U}) &=&  Var \left( \frac{1}{B} \sum_{i=1}^{B} U_i \right) \\
            &=&  \frac{\sigma^{2}}{B} \\
            & < & \sigma^{2}
\end{eqnarray*}
    \item This is a simple principle that is used here
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{\textcolor{brique}{[ Bagging ]}}
\textit{Bagging} stands for \textit{Bootstrap Aggregating} uses a simple logic:
\pause
\begin{enumerate}[<+->]
    \item Draw $B$ bootstrapped samples from the original sample
    \item Construct $B$ trees, one on each sample
    \item  \textit{Average} the result to get a prediction  with a lower variance
    \item[]
    \item[]In classification, we do not average but take the \emph{majority vote} rule
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{\textcolor{brique}{[ \textit{Bagging} ]}}
\textit{Bagging} scheme:
\includegraphics[width = 0.9\textwidth]{Ensemble_Bagging.svg.png}\\
\vfill
 \textcolor{gris}{\small Image from \textit{Par Sirakorn} (wikimedia.org) }
\end{frame}

% Out of bag prediction...

\section{Random Forest}


\begin{frame}
\frametitle{\textcolor{brique}{[ The problem with \textit{Bagging} ]}}
 Bootstrapped (\textit{Bagging}) trees use the same set of variables
\pause
\begin{itemize}[<+->]
    \item Aggregating  \textit{correlated} trees  will \textbf{not}  decrease variance
    \item   Intuitively, if $U_{i}, \ldots U_{B}$ are with variance $\sigma^{2}$, \textbf{and cross-correlation} $\rho$,   then the average has a variance:
\begin{eqnarray*}
Var ( \overline{U}) &=&  Var \left( \frac{1}{B} \sum_{i=1}^{B} U_i \right) \\
            &=&   \rho \cdot \sigma^{2} + \frac{\sigma^{2}}{B}
\end{eqnarray*}
    \item This could be "high" if $\rho$ is "high"
    \item[$\hookrightarrow$] \textit{Random forest }use a \textit{decorrelation} algorithm by adding \textit{\textbf{random}ness} in the set of variables used
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{\textcolor{brique}{[ Random Forest]}}
\textit{Random forest} use two mechanisms:
\pause
\begin{itemize}[<+->]
    \item Bootstrap aggregating (\textit{bagging})
    \item[$\hookrightarrow$] Construct $B$ trees, on $B$ bootstrapped samples
    \item \textbf{Random} selection of variables (\textit{Feature sampling})
    \item[$\hookrightarrow$] At each node, randomly select only \textbf{m} variables 
    \item The resulting predictor will have a lower variance
    $$
Var_{Random \; forest} = \rho \cdot \sigma^{2} + \frac{\sigma^{2}}{B}
$$

    \item[]Common practice ($p$ =\textit{ nb of variables}):
    \begin{itemize}
        \item $m \approx \sqrt{p}$ in classification
        \item $m \approx p/3$ in regression
    \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{\textcolor{brique}{[ Random Forest]}}
\textit{Random forest} scheme:
\begin{center}
\includegraphics[width = 0.85\textwidth]{Random_forest_diagram_complete.png}\\
\end{center}
 \textcolor{gris}{\small Image from  \textit{Venkata Jagannath} (wikimedia.org) }
\end{frame}


\begin{frame}
\frametitle{\textcolor{brique}{[ Optimizing Random Forest]}}
\textit{Random forest} are sensitive to several \textit{hyperparameters}
\pause
\begin{itemize}[<+->]
    \item All the parameters used to build a \textit{tree}
    \begin{itemize}[<+->]
        \item \textit{Purity} criterion (Gini \textit{vs} Entropy)
        \item Tree \textit{Depth} 
        \item \textit{Complexity} parameter $Cp$
        \item $\cdots$
    \end{itemize}
    \item Parameters used to construct the \textit{Forest}
    \begin{itemize}[<+->]
        \item Number of variables $m$  used in each node
        \item Number of trees 
        \item Minimum number of observations per leaves
        \item $\cdots$
    \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{\textcolor{brique}{[ Random Forest on an Example]}}
\pause
\begin{itemize}
\item[]
   \only<2> {\includegraphics[width = 0.8\textwidth]{mtryANDaccuracy-1.png}\\  }
   \only<2> {The impact of the number of variables (\textbf{\textit{m}}) on accuracy}
   \only<3> {\includegraphics[width = 0.8\textwidth]{mtryANDkappa-1.png}\\ }
   \only<3> {The impact of the number of variables (\textbf{\textit{m}}) on $kappa$ }
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{\textcolor{brique}{[ Random Forest on an Example]}}
The impact of the number of variables (\textbf{\textit{m}}):
\pause
\begin{itemize}[<+->]
    \item[] \includegraphics[width = 0.4\textwidth]{mtryANDaccuracy-1.png} \includegraphics[width = 0.4\textwidth]{mtryANDkappa-1.png}
    \item[] Why are the curves decreasing after a threshold?
    \item The variance depends on $\rho \cdot \sigma^{2}$ 
    \item[$\hookrightarrow$] \textit{Trees using similar variables are very similar (same predictions)} \\
    $\nearrow  m \Longrightarrow \rho \nearrow$\\  
    \item Optimum for \textbf{3} (out of 7) regressors only at each node
   \item[$\hookrightarrow$] Rule of thumb: $m \approx \sqrt{p}$ 
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{\textcolor{brique}{[ Random Forest on an Example]}}
\pause
\begin{itemize}
\item[]
   \only<2> {\includegraphics[width = 0.8\textwidth]{ntreeANDaccuracy-1.png}\\  }
   \only<2> {The impact of the number of trees on accuracy}
   \only<3> {\includegraphics[width = 0.8\textwidth]{ntreeANDkappa-1.png}\\ }
   \only<3> {The impact of the number of trees on $kappa$ }
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{\textcolor{brique}{[ Random Forest on an Example]}}
The impact of the number of trees:
\pause
\begin{itemize}[<+->]
    \item[] \includegraphics[width = 0.4\textwidth]{ntreeANDaccuracy-1.png} \includegraphics[width = 0.4\textwidth]{ntreeANDkappa-1.png}
    \item No clear optimum
    \item The variance depends on $\frac{\sigma^{2}}{B}$
    $\nearrow  B \Longrightarrow \frac{\sigma^{2}}{B}  \searrow $\\
   \item[$\hookrightarrow$] Improvement stop at some point
\end{itemize}
\end{frame}






\section{Boosting}



\end{document}
%%%%%%%%%%%%%%% Last Slide %%%%%%%%%%%%%%%%

\begin{frame}[allowframebreaks]%in case more than 1 slide needed
\frametitle{References}
    {\footnotesize
    %\bibliographystyle{authordate1}
    \bibliographystyle{apalike}
    \bibliography{../../../Visualisation/Visu}
    }
\end{frame}
\end{document}

%\bibliographystyle{authordate1}
%\bibliography{c:/Chris/Visualisation/Visu}
%\end{frame}
